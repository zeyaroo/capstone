{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lxml import html\n",
    "import requests\n",
    "import nltk\n",
    "#import stopwords from natural language toolkit\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "from itertools import groupby\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty dataframe\n",
    "df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop words\n",
    "stops = set(stopwords.words('english'))\n",
    "stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_counter(word_list):\n",
    "    wdict={}\n",
    "    for w in word_list:\n",
    "        if w in wdict:\n",
    "           wdict[w]=wdict[w]+1\n",
    "        else:\n",
    "            wdict[w]=1\n",
    "    #retun a dictionary with frequency count for each word\n",
    "    return wdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will first load the web page, scrape contents, remove white spaces, put individual words into a list, check against stopwords list, remove stopwords, special chars and numbers. Finally it will count each word and return a dictionary of frequency key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq_from_url(url,label,df):\n",
    "    str_=''\n",
    "    #load the webpage\n",
    "    #page = requests.get(url)\n",
    "    #xpath, parse through html\n",
    "    #tree = html.fromstring(page.content)\n",
    "    #p = tree.xpath('//p/text()')\n",
    "    #combine list items into a string\n",
    "    \n",
    "    os.environ['MOZ_HEADLESS'] = '1'\n",
    "    driver = webdriver.Firefox()\n",
    "    #driver.implicitly_wait(30)\n",
    "    try:\n",
    "        driver.get(url)    \n",
    "        #wait = WebDriverWait(driver, 1)\n",
    "        #wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"div\")))\n",
    "\n",
    "        driver.find_element_by_css_selector('.btn').click()\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html)\n",
    "        content = soup.find_all('div', {'data-rapid-subsec' : 'paragraph'})\n",
    "    except:\n",
    "        print('Error')\n",
    "        content=''\n",
    "    driver.quit()\n",
    "    for c in content:\n",
    "        str_ = str_ + c.text\n",
    "    #p=\" \".join(str)\n",
    "    punc=['!','.',',','(',')','?','{','}','[',']','\\\\','/','*','@','#','$','&','~','\"',':','_']\n",
    "    #split words by spaces and converted them into a list\n",
    "    p=str_.split(\" \")\n",
    "    wlist=[]\n",
    "    for word in p:\n",
    "        if word.lower().strip() not in stops:\n",
    "            #leave out numbers, check if the characters are alphabets\n",
    "            for p in punc:\n",
    "                    word=word.replace(p,\"\")\n",
    "            #if alphabet, put it into wlist; if digit, skip\n",
    "            if word.strip().isalpha()==True:\n",
    "                if word.strip() != \"\":\n",
    "                    wlist.append(word)\n",
    "    #get term frequency as a dictionary\n",
    "    #tf_dict = {value: len(list(freq)) for value, freq in groupby(sorted(wlist))}\n",
    "    tf_dict=freq_counter(wlist)\n",
    "    tf_dict['label']=label\n",
    "    df2  = pd.DataFrame([tf_dict], columns=tf_dict.keys())\n",
    "    df = pd.concat([df, df2], axis =0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_freq_from_string(string,label,df):\n",
    "\n",
    "    punc=['!','.',',','(',')','?','{','}','[',']','\\\\','/','*','@','#','$','&','~','\"',':','_']\n",
    "    #split words by spaces and converted them into a list\n",
    "    p=string.split(\" \")\n",
    "    wlist=[]\n",
    "    for word in p:\n",
    "        if word.lower().strip() not in stops:\n",
    "            #leave out numbers, check if the characters are alphabets\n",
    "            for p in punc:\n",
    "                    word=word.replace(p,\"\")\n",
    "            #if alphabet, put it into wlist; if digit, skip\n",
    "            if word.strip().isalpha()==True:\n",
    "                if word.strip() != \"\":\n",
    "                    wlist.append(word)\n",
    "    #get term frequency as a dictionary\n",
    "    #tf_dict = {value: len(list(freq)) for value, freq in groupby(sorted(wlist))}\n",
    "    tf_dict=freq_counter(wlist)\n",
    "    tf_dict['label']=label\n",
    "    df2  = pd.DataFrame([tf_dict], columns=tf_dict.keys())\n",
    "    df = pd.concat([df, df2], axis =0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will pass a list of URLs with label for each story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tedious to do it manually. So I am using a data set created by Rishabh Misra. It contains over 200,000 articles from Huffington post with category label, headline, author names, link, short description and date. You can download the dataset at https://www.kaggle.com/rmisra/news-category-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>TECH</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             category                                           headline  \\\n",
       "0               CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1       ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2       ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3       ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4       ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "...               ...                                                ...   \n",
       "200848           TECH  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
       "200849         SPORTS  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
       "200850         SPORTS  Giants Over Patriots, Jets Over Colts Among  M...   \n",
       "200851         SPORTS  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
       "200852         SPORTS  Dwight Howard Rips Teammates After Magic Loss ...   \n",
       "\n",
       "                 authors                                               link  \\\n",
       "0        Melissa Jeltsen  https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1          Andy McDonald  https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2             Ron Dicker  https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3             Ron Dicker  https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4             Ron Dicker  https://www.huffingtonpost.com/entry/julianna-...   \n",
       "...                  ...                                                ...   \n",
       "200848  Reuters, Reuters  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "200849                    https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "200850                    https://www.huffingtonpost.com/entry/super-bow...   \n",
       "200851                    https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "200852                    https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                        short_description       date  \n",
       "0       She left her husband. He killed their children... 2018-05-26  \n",
       "1                                Of course it has a song. 2018-05-26  \n",
       "2       The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3       The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4       The \"Dietland\" actress said using the bags is ... 2018-05-26  \n",
       "...                                                   ...        ...  \n",
       "200848  Verizon Wireless and AT&T are already promotin... 2012-01-28  \n",
       "200849  Afterward, Azarenka, more effusive with the pr... 2012-01-28  \n",
       "200850  Leading up to Super Bowl XLVI, the most talked... 2012-01-28  \n",
       "200851  CORRECTION: An earlier version of this story i... 2012-01-28  \n",
       "200852  The five-time all-star center tore into his te... 2012-01-28  \n",
       "\n",
       "[200853 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "articles=pd.read_json(\"News_Category_Dataset_v2.json\",lines=True)\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are over 200,000 articles in this data set. The whole set can be used to train the model but it will take a very long time on my laptop. So I will make a smaller subset by selecting random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMPACT</td>\n",
       "      <td>The Countdown Starts Now</td>\n",
       "      <td>Sarah Brown, ContributorPresident of Theirworl...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-count...</td>\n",
       "      <td>Despite significant progress allowing tens of ...</td>\n",
       "      <td>2014-08-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "      <td>On Retreat</td>\n",
       "      <td>Mark Nepo, ContributorAuthor, New York Times b...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/on-retrea...</td>\n",
       "      <td>It's humbling how fear can rearrange our eyes....</td>\n",
       "      <td>2014-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>The Alphabet of Happiness: 'G'</td>\n",
       "      <td>Randy Taran, Contributor\\nFounder of projectha...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-alpha...</td>\n",
       "      <td>Today let's look at Giving, Grace, and sharing...</td>\n",
       "      <td>2013-06-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>California Ballot Measure Fails To Create Many...</td>\n",
       "      <td>JULIA HOROWITZ, AP</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/californi...</td>\n",
       "      <td>Only 1,700 jobs have been created in three years.</td>\n",
       "      <td>2015-08-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>Most Notable Quote Of 2016? Trump Saying He Co...</td>\n",
       "      <td>Claire Fallon</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/trump-quo...</td>\n",
       "      <td>That about sums up this year.</td>\n",
       "      <td>2016-12-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200848</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>Education Policies Worthy of the Name 'Reform'</td>\n",
       "      <td>Randi Weingarten, ContributorPresident, Americ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/education...</td>\n",
       "      <td>This new law can create positive change. State...</td>\n",
       "      <td>2015-12-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200849</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>Smart Travel Advice: The More You Travel, the ...</td>\n",
       "      <td>Christopher Elliott, Contributor\\nAuthor, How ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/smart-tra...</td>\n",
       "      <td>\"The world is essentially a friendly place and...</td>\n",
       "      <td>2014-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200850</th>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Washington's NFL Team Is In Desperate Need Of ...</td>\n",
       "      <td>Kim Bellware</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/washingto...</td>\n",
       "      <td>The owner who chose the team's controversial n...</td>\n",
       "      <td>2016-04-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200851</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>The Democrats' 'Working-Class Problem'</td>\n",
       "      <td>The American Prospect, ContributorLiberal Inte...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-democ...</td>\n",
       "      <td>The Democrats also missed the economic stress ...</td>\n",
       "      <td>2017-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200852</th>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Migraines Linked To Structural Changes In Brain</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/migraine-...</td>\n",
       "      <td>The study, published in the journal Neurology,...</td>\n",
       "      <td>2013-08-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200853 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              category                                           headline  \\\n",
       "0               IMPACT                           The Countdown Starts Now   \n",
       "1       HEALTHY LIVING                                         On Retreat   \n",
       "2             WELLNESS                     The Alphabet of Happiness: 'G'   \n",
       "3             POLITICS  California Ballot Measure Fails To Create Many...   \n",
       "4       ARTS & CULTURE  Most Notable Quote Of 2016? Trump Saying He Co...   \n",
       "...                ...                                                ...   \n",
       "200848       EDUCATION     Education Policies Worthy of the Name 'Reform'   \n",
       "200849          TRAVEL  Smart Travel Advice: The More You Travel, the ...   \n",
       "200850          SPORTS  Washington's NFL Team Is In Desperate Need Of ...   \n",
       "200851        POLITICS             The Democrats' 'Working-Class Problem'   \n",
       "200852        WELLNESS    Migraines Linked To Structural Changes In Brain   \n",
       "\n",
       "                                                  authors  \\\n",
       "0       Sarah Brown, ContributorPresident of Theirworl...   \n",
       "1       Mark Nepo, ContributorAuthor, New York Times b...   \n",
       "2       Randy Taran, Contributor\\nFounder of projectha...   \n",
       "3                                      JULIA HOROWITZ, AP   \n",
       "4                                           Claire Fallon   \n",
       "...                                                   ...   \n",
       "200848  Randi Weingarten, ContributorPresident, Americ...   \n",
       "200849  Christopher Elliott, Contributor\\nAuthor, How ...   \n",
       "200850                                       Kim Bellware   \n",
       "200851  The American Prospect, ContributorLiberal Inte...   \n",
       "200852                                                      \n",
       "\n",
       "                                                     link  \\\n",
       "0       https://www.huffingtonpost.com/entry/the-count...   \n",
       "1       https://www.huffingtonpost.com/entry/on-retrea...   \n",
       "2       https://www.huffingtonpost.com/entry/the-alpha...   \n",
       "3       https://www.huffingtonpost.com/entry/californi...   \n",
       "4       https://www.huffingtonpost.com/entry/trump-quo...   \n",
       "...                                                   ...   \n",
       "200848  https://www.huffingtonpost.com/entry/education...   \n",
       "200849  https://www.huffingtonpost.com/entry/smart-tra...   \n",
       "200850  https://www.huffingtonpost.com/entry/washingto...   \n",
       "200851  https://www.huffingtonpost.com/entry/the-democ...   \n",
       "200852  https://www.huffingtonpost.com/entry/migraine-...   \n",
       "\n",
       "                                        short_description       date  \n",
       "0       Despite significant progress allowing tens of ... 2014-08-18  \n",
       "1       It's humbling how fear can rearrange our eyes.... 2014-12-02  \n",
       "2       Today let's look at Giving, Grace, and sharing... 2013-06-06  \n",
       "3       Only 1,700 jobs have been created in three years. 2015-08-17  \n",
       "4                           That about sums up this year. 2016-12-16  \n",
       "...                                                   ...        ...  \n",
       "200848  This new law can create positive change. State... 2015-12-20  \n",
       "200849  \"The world is essentially a friendly place and... 2014-02-21  \n",
       "200850  The owner who chose the team's controversial n... 2016-04-28  \n",
       "200851  The Democrats also missed the economic stress ... 2017-06-02  \n",
       "200852  The study, published in the journal Neurology,... 2013-08-28  \n",
       "\n",
       "[200853 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffling the rows\n",
    "articles = articles.sample(frac=1).reset_index(drop=True)\n",
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting first 500 articles from the list\n",
    "articles2=articles[:500]\n",
    "articles2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bryner</th>\n",
       "      <th>Care</th>\n",
       "      <th>Health</th>\n",
       "      <th>Hill</th>\n",
       "      <th>Lobbyists</th>\n",
       "      <th>Sarah</th>\n",
       "      <th>Strangers</th>\n",
       "      <th>aides</th>\n",
       "      <th>approving</th>\n",
       "      <th>care</th>\n",
       "      <th>...</th>\n",
       "      <th>Four</th>\n",
       "      <th>Haiku</th>\n",
       "      <th>Recap</th>\n",
       "      <th>none</th>\n",
       "      <th>Creating</th>\n",
       "      <th>Kiss</th>\n",
       "      <th>Perfection</th>\n",
       "      <th>kiss</th>\n",
       "      <th>lifetime</th>\n",
       "      <th>seal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 4912 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bryner  Care  Health  Hill  Lobbyists  Sarah  Strangers  aides  \\\n",
       "0       1.0   1.0     1.0   1.0        1.0    1.0        1.0    1.0   \n",
       "1       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "2       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "3       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "4       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "..      ...   ...     ...   ...        ...    ...        ...    ...   \n",
       "495     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "496     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "497     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "498     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "499     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "\n",
       "     approving  care  ...  Four  Haiku  Recap  none  Creating  Kiss  \\\n",
       "0          1.0   1.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "1          0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "2          0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "3          0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "4          0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "..         ...   ...  ...   ...    ...    ...   ...       ...   ...   \n",
       "495        0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "496        0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "497        0.0   0.0  ...   0.0    0.0    0.0   0.0       0.0   0.0   \n",
       "498        0.0   0.0  ...   1.0    1.0    1.0   1.0       0.0   0.0   \n",
       "499        0.0   0.0  ...   0.0    0.0    0.0   0.0       1.0   1.0   \n",
       "\n",
       "     Perfection  kiss  lifetime seal  \n",
       "0           0.0   0.0       0.0  0.0  \n",
       "1           0.0   0.0       0.0  0.0  \n",
       "2           0.0   0.0       0.0  0.0  \n",
       "3           0.0   0.0       0.0  0.0  \n",
       "4           0.0   0.0       0.0  0.0  \n",
       "..          ...   ...       ...  ...  \n",
       "495         0.0   0.0       0.0  0.0  \n",
       "496         0.0   0.0       0.0  0.0  \n",
       "497         0.0   0.0       0.0  0.0  \n",
       "498         0.0   0.0       0.0  0.0  \n",
       "499         1.0   2.0       1.0  1.0  \n",
       "\n",
       "[500 rows x 4912 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "#breaking up 3000 into 6 different loops. For some reason, it is faster than running 3000 at once.\n",
    "#for x in range(1,6):\n",
    "#    articles2=articles[500*(x-1):500*x]\n",
    "for index, row in articles2.iterrows():\n",
    "    #print(row['category'], row['link'])\n",
    "    df=get_term_freq_from_string(row['headline']+\" \"+row['short_description'],row['category'],df)\n",
    "    df=df.fillna(0).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bryner</th>\n",
       "      <th>Care</th>\n",
       "      <th>Health</th>\n",
       "      <th>Hill</th>\n",
       "      <th>Lobbyists</th>\n",
       "      <th>Sarah</th>\n",
       "      <th>Strangers</th>\n",
       "      <th>aides</th>\n",
       "      <th>approving</th>\n",
       "      <th>care</th>\n",
       "      <th>...</th>\n",
       "      <th>Amid</th>\n",
       "      <th>Lecture</th>\n",
       "      <th>conservative</th>\n",
       "      <th>lecture</th>\n",
       "      <th>pundit</th>\n",
       "      <th>vowed</th>\n",
       "      <th>Means</th>\n",
       "      <th>deductions</th>\n",
       "      <th>papers</th>\n",
       "      <th>refund</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 7884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Bryner  Care  Health  Hill  Lobbyists  Sarah  Strangers  aides  \\\n",
       "0       1.0   1.0     1.0   1.0        1.0    1.0        1.0    1.0   \n",
       "1       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "2       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "3       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "4       0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "..      ...   ...     ...   ...        ...    ...        ...    ...   \n",
       "994     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "995     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "996     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "997     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "998     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "\n",
       "     approving  care  ...  Amid  Lecture  conservative  lecture  pundit  \\\n",
       "0          1.0   1.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "1          0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "2          0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "3          0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "4          0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "..         ...   ...  ...   ...      ...           ...      ...     ...   \n",
       "994        0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "995        0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "996        0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "997        0.0   0.0  ...   1.0      1.0           1.0      1.0     1.0   \n",
       "998        0.0   0.0  ...   0.0      0.0           0.0      0.0     0.0   \n",
       "\n",
       "     vowed  Means  deductions  papers refund  \n",
       "0      0.0    0.0         0.0     0.0    0.0  \n",
       "1      0.0    0.0         0.0     0.0    0.0  \n",
       "2      0.0    0.0         0.0     0.0    0.0  \n",
       "3      0.0    0.0         0.0     0.0    0.0  \n",
       "4      0.0    0.0         0.0     0.0    0.0  \n",
       "..     ...    ...         ...     ...    ...  \n",
       "994    0.0    0.0         0.0     0.0    0.0  \n",
       "995    0.0    0.0         0.0     0.0    0.0  \n",
       "996    0.0    0.0         0.0     0.0    0.0  \n",
       "997    1.0    0.0         0.0     0.0    0.0  \n",
       "998    0.0    1.0         1.0     1.0    1.0  \n",
       "\n",
       "[999 rows x 7884 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles2=articles[501:1000]\n",
    "for index, row in articles2.iterrows():\n",
    "    #print(row['category'], row['link'])\n",
    "    df=get_term_freq_from_string(row['headline']+\" \"+row['short_description'],row['category'],df)\n",
    "    df=df.fillna(0).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bryner</th>\n",
       "      <th>Care</th>\n",
       "      <th>Health</th>\n",
       "      <th>Hill</th>\n",
       "      <th>Lobbyists</th>\n",
       "      <th>Sarah</th>\n",
       "      <th>Strangers</th>\n",
       "      <th>aides</th>\n",
       "      <th>approving</th>\n",
       "      <th>care</th>\n",
       "      <th>...</th>\n",
       "      <th>Colombian</th>\n",
       "      <th>Fully</th>\n",
       "      <th>Legalizing</th>\n",
       "      <th>Toward</th>\n",
       "      <th>illegal</th>\n",
       "      <th>incest</th>\n",
       "      <th>Classic</th>\n",
       "      <th>Solo</th>\n",
       "      <th>predict</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HOME &amp; LIVING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PARENTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MEDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>LATINO VOICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1346 rows × 9690 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bryner  Care  Health  Hill  Lobbyists  Sarah  Strangers  aides  \\\n",
       "0        1.0   1.0     1.0   1.0        1.0    1.0        1.0    1.0   \n",
       "1        0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "2        0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "3        0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "4        0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "...      ...   ...     ...   ...        ...    ...        ...    ...   \n",
       "1341     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "1342     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "1343     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "1344     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "1345     0.0   0.0     0.0   0.0        0.0    0.0        0.0    0.0   \n",
       "\n",
       "      approving  care  ...  Colombian  Fully  Legalizing  Toward  illegal  \\\n",
       "0           1.0   1.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "1           0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "2           0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "3           0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "4           0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "...         ...   ...  ...        ...    ...         ...     ...      ...   \n",
       "1341        0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "1342        0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "1343        0.0   0.0  ...        2.0    1.0         1.0     1.0      1.0   \n",
       "1344        0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "1345        0.0   0.0  ...        0.0    0.0         0.0     0.0      0.0   \n",
       "\n",
       "      incest  Classic  Solo  predict          label  \n",
       "0        0.0      0.0   0.0      0.0       POLITICS  \n",
       "1        0.0      0.0   0.0      0.0  HOME & LIVING  \n",
       "2        0.0      0.0   0.0      0.0        PARENTS  \n",
       "3        0.0      0.0   0.0      0.0           TECH  \n",
       "4        0.0      0.0   0.0      0.0      PARENTING  \n",
       "...      ...      ...   ...      ...            ...  \n",
       "1341     0.0      0.0   0.0      0.0       POLITICS  \n",
       "1342     0.0      0.0   0.0      0.0          MEDIA  \n",
       "1343     1.0      0.0   0.0      0.0  LATINO VOICES  \n",
       "1344     0.0      1.0   2.0      0.0  ENTERTAINMENT  \n",
       "1345     0.0      0.0   0.0      1.0       POLITICS  \n",
       "\n",
       "[1346 rows x 9690 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "df3.drop(['label'],axis=1,inplace=True)\n",
    "df3 = df3.replace(0, np.nan)\n",
    "df3 = df3.dropna(how='all', axis=0)\n",
    "df3 = df3.replace(np.nan, 0)\n",
    "df3['label']=df['label']\n",
    "df3=df3.reset_index(drop=True)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc=pd.read_csv(\"Downloads/bbc.csv\")\n",
    "bbc_500=bbc[:500]\n",
    "for index, row in bbc_500.iterrows():\n",
    "    #print(row['category'], row['link'])\n",
    "    df=get_term_freq_from_string(row['news'],row['type'],df)\n",
    "    df=df.fillna(0).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(column='label', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['label'].values\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "ConfustionMx = [];\n",
    "for n in range(1,Ks):\n",
    "    \n",
    "    #Train Model and Predict  \n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    yhat=neigh.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "\n",
    "    \n",
    "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "mean_acc\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\n",
    "print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,Ks),mean_acc,'g')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Nabors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = neigh.predict(X_test)\n",
    "yhat[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
